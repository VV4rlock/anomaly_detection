{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AnoGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyUelMCMJBvl"
      },
      "source": [
        "!pip3 install tensorboardx\n",
        "    \n",
        "import datetime, os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid\n",
        "import pickle\n",
        "import cv2\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import zipfile\n",
        "import gc\n",
        "import math, os\n",
        "from random import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p0F8gUrJLMb",
        "outputId": "f941fdcd-7967-4632-f319-040c93b90362"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/datasets/data_ad_anogan.zip ./\n",
        "!7z x data_ad_anogan.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 680158946 bytes (649 MiB)\n",
            "\n",
            "Extracting archive: data_ad_anogan.zip\n",
            "--\n",
            "Path = data_ad_anogan.zip\n",
            "Type = zip\n",
            "Physical Size = 680158946\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b  7% 103 - data/capsule/test/scratch/022.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 12% 167\b\b\b\b\b\b\b\b        \b\b\b\b\b\b\b\b 19% 240 - data/capsule/train/good/144.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% 304 - data/pill/train/good/033.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% 372 - data/pill/test/contamination/019.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 440 - data/capsule/train/good/143.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% 499 - data/capsule/train/good/028.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 564 - data/pill/test/good/008.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% 630 - data/capsule/train/good/103.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 694 - data/capsule/train/good/003.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 746 - data/capsule/train/good/137.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% 811 - data/pill/train/good/217.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 879 - data/capsule/train/good/129.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 950 - data/capsule/train/good/112.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 1020 - data/capsule/train/good/015.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Files: 1040\n",
            "Size:       679831577\n",
            "Compressed: 680158946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PO3Z1xyRboeb"
      },
      "source": [
        "class PillsDataset(Dataset):\n",
        "    def __init__(self, type='train', _transforms=None):\n",
        "        assert type in ['test', 'train', 'ground_truth']\n",
        "        self.type = type\n",
        "        self.transforms = _transforms\n",
        "        self.samples = []\n",
        "        self.toPIL = torchvision.transforms.ToPILImage()\n",
        "        self.toTensor = torchvision.transforms.ToTensor()\n",
        "        for root,d_names,f_names in os.walk(\"/content/data/capsule/\"+type+\"/good\"):\n",
        "          for _file in f_names:\n",
        "            filename = os.path.join(root, _file)\n",
        "            self.samples.append(filename)\n",
        "        print(f\"dataset_len: {len(self.samples)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        name = self.samples[index]\n",
        "        sample = Image.open(name).convert('RGB')\n",
        "        if self.transforms:\n",
        "            sample = self.transforms(sample)\n",
        "        return sample\n",
        "\n",
        "BS = 64\n",
        "train_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "         \n",
        "            #transforms.ColorJitter(saturation=0.3, contrast=0.3, brightness=0.3),\n",
        "            transforms.ToTensor(),\n",
        "            #transforms.Normalize((123.68 / 255, 116.779 / 255, 103.939/ 255), (58.393 / 255, 57.12 / 255, 57.375 / 255)),\n",
        "            transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
        "        ]\n",
        "    )\n",
        "trainset = PillsDataset('train', _transforms=train_transform)\n",
        "train_loader = DataLoader(trainset, batch_size=BS, shuffle=True, drop_last=True)\n",
        "\n",
        "def prin_batch():\n",
        "    TRAIN=1\n",
        "\n",
        "    \n",
        "    epochs = 0\n",
        "    epoch = 0\n",
        "    for images in train_loader:\n",
        "      epoch+=1\n",
        "      #trainset.get_label(label_indices)\n",
        "      break\n",
        "    \n",
        "    img = images.data \n",
        "    img = img - img.min()\n",
        "    img = img / img.max()\n",
        "    img = make_grid(img, nrow=2, padding=1).numpy()\n",
        "    img = np.rollaxis(img, 0, 3)\n",
        "\n",
        "    plt.figure(figsize=(100, 100)), plt.imshow(img), plt.show()\n",
        "    if False:\n",
        "        plt.figure(figsize=(14, 5)), plt.imshow(img[:,:,0]), plt.show()\n",
        "        plt.figure(figsize=(14, 5)), plt.imshow(img[:,:,1]), plt.show()\n",
        "        plt.figure(figsize=(14, 5)), plt.imshow(img[:,:,2]), plt.show()\n",
        "\n",
        "prin_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYR_z9HSmFB5",
        "outputId": "977addee-c369-4f6a-cbdc-7055a6fa4b79"
      },
      "source": [
        "def MyConvTranspose(in_channel, out_channel):\n",
        "  conv = nn.ConvTranspose2d(in_channel, out_channel, (2, 2), (2, 2), (0, 0), bias=False)\n",
        "  nn.init.normal_(conv.weight.data, 0, 0.02)\n",
        "\n",
        "  bn = nn.BatchNorm2d(out_channel, affine=True, track_running_stats=False)\n",
        "  #torch.nn.init.zeros_(bn.weight)\n",
        "  nn.init.normal_(bn.weight.data, 1.0, 0.02)\n",
        "  nn.init.constant_(bn.bias.data, 0)\n",
        "\n",
        "  seq = nn.Sequential(\n",
        "    conv,\n",
        "    bn,\n",
        "    nn.ReLU()\n",
        "  )\n",
        "  return seq\n",
        "\n",
        "\n",
        "class DC_generator(nn.Module):\n",
        "  def __init__(self, latent_vector_size=100):\n",
        "    super(DC_generator, self).__init__()\n",
        "    self.latent_to_tensor = nn.Linear(latent_vector_size, 4*4*1024, bias=False)\n",
        "    torch.nn.init.xavier_normal_(self.latent_to_tensor.weight)\n",
        "\n",
        "    lastconv = nn.ConvTranspose2d(32, 3, (2, 2), (2, 2), (0, 0))\n",
        "    nn.init.normal_(lastconv.weight.data, 0, 0.02)\n",
        "\n",
        "    last_bn = nn.BatchNorm2d(3, affine=True, track_running_stats=False)\n",
        "    nn.init.normal_(last_bn.weight.data, 1.0, 0.02)\n",
        "    nn.init.constant_(last_bn.bias.data, 0)\n",
        "\n",
        "    fbn = nn.BatchNorm2d(1024, affine=True, track_running_stats=False)\n",
        "    \n",
        "    nn.init.normal_(fbn.weight.data, 1.0, 0.02)\n",
        "    nn.init.constant_(fbn.bias.data, 0)\n",
        "    \n",
        "    layers = [\n",
        "                fbn,\n",
        "                nn.ReLU(),\n",
        "                MyConvTranspose(1024, 512),\n",
        "                MyConvTranspose(512, 256),\n",
        "                MyConvTranspose(256, 128),\n",
        "                MyConvTranspose(128, 64),\n",
        "                MyConvTranspose(64, 32),\n",
        "                lastconv,  \n",
        "                last_bn,\n",
        "                nn.Tanh()        \n",
        "             ]\n",
        "    self.layers = nn.Sequential(\n",
        "            *layers\n",
        "        )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.latent_to_tensor(x)\n",
        "    x = x.view(x.size(0), 1024, 4, 4)\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "\n",
        "def MyConv(in_channel, out_channel):\n",
        "  conv = nn.Conv2d(in_channel, out_channel, (2, 2), (2, 2), (0, 0), bias=False)\n",
        "  nn.init.normal_(conv.weight.data, 0, 0.02)\n",
        "\n",
        "  bn = nn.BatchNorm2d(out_channel, affine=True, track_running_stats=False)\n",
        "  nn.init.normal_(bn.weight.data, 1.0, 0.02)\n",
        "  nn.init.constant_(bn.bias.data, 0)\n",
        "\n",
        "  conv = nn.Sequential(\n",
        "    conv,\n",
        "    bn,\n",
        "    nn.ReLU()\n",
        "  )\n",
        "  return conv\n",
        "\n",
        "class DC_discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DC_discriminator, self).__init__()\n",
        "    layers = [\n",
        "                MyConv(3,32),\n",
        "                MyConv(32,64),\n",
        "                MyConv(64,128),\n",
        "                MyConv(128,256),\n",
        "                MyConv(256,512), \n",
        "                MyConv(512,1024),        \n",
        "             ]\n",
        "    self.layers = nn.Sequential(\n",
        "            *layers\n",
        "        )\n",
        "    self.linear = nn.Linear(4*4*1024, 1, bias=False)\n",
        "    torch.nn.init.xavier_normal_(self.linear.weight)\n",
        "    \n",
        "    \n",
        "  def forward(self, x):\n",
        "    #x = self.latent_to_tensor(x)\n",
        "    #x = x.view(x.size(0), 1024, 4, 4)\n",
        "    x = self.layers(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.linear(x)\n",
        "    return torch.sigmoid(x)\n",
        "\n",
        "    \n",
        "_input = torch.rand((2,100))\n",
        "gen = DC_generator(100)\n",
        "descr = DC_discriminator()\n",
        "out = gen(_input)\n",
        "print(out.size())\n",
        "print(descr(out).size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 256, 256])\n",
            "torch.Size([2, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXP3YHleERwo"
      },
      "source": [
        "G_LR = 0.0002\n",
        "D_LR = 0.0002\n",
        "CUDA = torch.cuda.is_available()\n",
        "LATENT_DIM = 100\n",
        "EPOCHS_COUNT = 1000\n",
        " \n",
        "def denormalize(x):\n",
        "  std = torch.tensor((58.393 / 255, 57.12 / 255, 57.375 / 255)).view(-1,1,1)\n",
        "  mean = torch.tensor((123.68 / 255, 116.779 / 255, 103.939 / 255)).view(-1,1,1)\n",
        "  if CUDA:\n",
        "    std = std.cuda()\n",
        "    mean= mean.cuda()\n",
        "  out = x.mul_(std).add_(mean)\n",
        "  out[out>1] = 1\n",
        "  out[out<0] = 0\n",
        "  return out\n",
        " \n",
        " \n",
        "def save_images(batch, epoch):\n",
        "  path_to, k = \"generated_images/\", 0\n",
        "  if not os.path.exists(path_to):\n",
        "      os.makedirs(path_to)\n",
        "  batch = denormalize(batch)\n",
        "  for image in batch:\n",
        "      torchvision.utils.save_image(image, f\"{path_to}{epoch}.jpg\",\n",
        "                                  normalize=False)\n",
        "      break\n",
        "!rm -rf /content/generated_images\n",
        "def train_model():\n",
        " \n",
        "  generator = DC_generator(LATENT_DIM)\n",
        "  discriminator = DC_discriminator()\n",
        "  if CUDA:\n",
        "    generator = generator.cuda()\n",
        "    discriminator = discriminator.cuda()\n",
        "  \n",
        "  optimizer_G = torch.optim.Adam(generator.parameters(), lr=G_LR, betas=(0.5, 0.999),)\n",
        "  optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=D_LR, betas=(0.5, 0.999),)\n",
        " \n",
        "  CEloss = nn.BCELoss()\n",
        " \n",
        "  ones = torch.ones((BS,1))\n",
        "  zeros = torch.zeros((BS,1))\n",
        " \n",
        "  if CUDA:\n",
        "    ones = ones.cuda()\n",
        "    zeros = zeros.cuda()\n",
        " \n",
        "  for epoch in range(EPOCHS_COUNT):\n",
        "    print(f\"Epoch {epoch}/{EPOCHS_COUNT}\")\n",
        "    iter_count = len(train_loader)\n",
        "    g_losses = []\n",
        "    real_acc = []\n",
        "    fake_acc = []\n",
        "    d_losses = []\n",
        "    real_losses = []\n",
        "    fake_losses = []\n",
        "\n",
        "    g_images = None\n",
        "    for i, images in enumerate(train_loader):\n",
        "      \n",
        "      z = torch.rand((images.size(0), LATENT_DIM))\n",
        "      if CUDA:\n",
        "        images = images.cuda()\n",
        "        z = z.cuda()\n",
        " \n",
        "      n_critic = 1\n",
        "      clip_param = 0 #0.01\n",
        "      if 1: #i % n_critic == 0:\n",
        "        optimizer_G.zero_grad()\n",
        "        g_images = generator(z)\n",
        "        g_loss = CEloss(discriminator(g_images), ones)\n",
        "        #g_loss = -torch.mean(discriminator(g_images))\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        " \n",
        "      optimizer_D.zero_grad()\n",
        "\n",
        "      d_fake_out = discriminator(g_images.detach())\n",
        "      d_fake_loss = CEloss(d_fake_out, zeros)\n",
        "      #d_fake_loss = torch.mean(d_fake_out)\n",
        "      d_fake_loss.backward()\n",
        "\n",
        "      optimizer_D.step()\n",
        "      optimizer_D.zero_grad()\n",
        "\n",
        "      d_real_out = discriminator(images)\n",
        "      d_real_loss = CEloss(d_real_out, ones)\n",
        "      #d_real_loss = -torch.mean(d_real_out)\n",
        "\n",
        "      d_loss = (d_fake_loss + 0.1*d_real_loss)\n",
        "      d_real_loss.backward()\n",
        "      \n",
        "      #d_loss.backward()\n",
        " \n",
        "      optimizer_D.step()\n",
        " \n",
        "      \n",
        "      '''\n",
        "      #wesserstein clipping\n",
        "      if clip_param:\n",
        "        for p in discriminator.parameters():\n",
        "              p.data.clamp_(-clip_param, clip_param)\n",
        "      '''\n",
        "\n",
        "      g_losses.append(g_loss.item())\n",
        "      d_losses.append(d_loss.item())\n",
        " \n",
        "      real_acc.append(torch.mean(d_real_out).item())\n",
        "      fake_acc.append(torch.mean(d_fake_out).item())\n",
        "      real_losses.append(d_real_loss.item())\n",
        "      fake_losses.append(d_fake_loss.item())\n",
        "      print(f\"\\r\\titeration {i+1}/{iter_count}: real_out: {real_acc[-1]}, fake_out: {fake_acc[-1]} g_loss: {g_losses[-1]}, real_loss: {real_losses[-1]}, fake_loss: {fake_losses[-1]}\")\n",
        "    avg = lambda x: sum(x)/len(x)\n",
        "    save_images(g_images, epoch)\n",
        "    print(f\"  Epoch {epoch} result avg: real_out: {avg(real_acc)}, fake_out: {avg(fake_acc)} g_loss: {avg(g_losses)}, real_loss: {avg(real_losses)}, fake_loss: {avg(fake_losses)}\")\n",
        " \n",
        " \n",
        "train_model()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}